{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, RNN, SimpleRNNCell, SimpleRNN,Dropout, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aalst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarika</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0  Aaberg\n",
       "1   Aalst\n",
       "2    Aara\n",
       "3   Aaren\n",
       "4  Aarika"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attention Layer\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    " \n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None, timesteps=None):\n",
    "    '''Apply y.w + b for every temporal slice y of x.\n",
    "    '''\n",
    "    if not input_dim:\n",
    "        # won't work with TensorFlow\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        # won't work with TensorFlow\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        # won't work with TensorFlow\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x *= expanded_dropout_matrix\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "\n",
    "    x = K.dot(x, w)\n",
    "    if b:\n",
    "        x = x + b\n",
    "    # reshape to 3D tensor\n",
    "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    " \n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    " \n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    " \n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    " \n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    " \n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    " \n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    " \n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    " \n",
    "        self.states = [None, None]  # y, s\n",
    " \n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    " \n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    " \n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    " \n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    " \n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    " \n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    " \n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    " \n",
    "        return super(AttentionDecoder, self).call(x)\n",
    " \n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    " \n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    " \n",
    "        return [y0, s0]\n",
    " \n",
    "    def step(self, x, states):\n",
    " \n",
    "        ytm, stm = states\n",
    " \n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    " \n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    " \n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    " \n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    " \n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    " \n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    " \n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    " \n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    " \n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    " \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    " \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "#Load data\n",
    "companies = pd.read_csv('names.txt', header=None)\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 56\n"
     ]
    }
   ],
   "source": [
    "names = companies[0].values\n",
    "text = '\\n'.join(names)\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars: {}'.format(len(chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 52450\n",
      "First 10 sequences and next chars:\n",
      "[Aaberg\n",
      "Aal]:[s]\n",
      "[erg\n",
      "Aalst\n",
      "]:[A]\n",
      "[\n",
      "Aalst\n",
      "Aar]:[a]\n",
      "[lst\n",
      "Aara\n",
      "A]:[a]\n",
      "[\n",
      "Aara\n",
      "Aare]:[n]\n",
      "[ra\n",
      "Aaren\n",
      "A]:[a]\n",
      "[Aaren\n",
      "Aari]:[k]\n",
      "[en\n",
      "Aarika\n",
      "]:[A]\n",
      "[Aarika\n",
      "Aar]:[o]\n",
      "[ika\n",
      "Aaron\n",
      "]:[A]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('[{}]:[{}]'.format(sentences[i], next_chars[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Size of X: 28.00 MB\n",
      "Size of y: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Size of X: {:.2f} MB'.format(X.nbytes/1024/1024))\n",
    "print('Size of y: {:.2f} MB'.format(y.nbytes/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 10, 64)            30976     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 10, 64)            185406    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 56)                3640      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 56)                0         \n",
      "=================================================================\n",
      "Total params: 244,790\n",
      "Trainable params: 244,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ### Initialization\n",
    "# \n",
    "# Now we are ready to create a recurrent model.  Keras contains three types of recurrent layers:\n",
    "# \n",
    "#  * `SimpleRNN`, a fully-connected RNN where the output is fed back to input.\n",
    "#  * `LSTM`, the Long-Short Term Memory unit layer.\n",
    "#  * `GRU`, the Gated Recurrent Unit layer.\n",
    "# \n",
    "# See https://keras.io/layers/recurrent/ for more information.\n",
    "\n",
    "# Number of hidden units to use:\n",
    "nb_units = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Recurrent layers supported: SimpleRNN, LSTM, GRU:\n",
    "\n",
    "model.add(LSTM(nb_units, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "model.add(AttentionDecoder(150, nb_units))\n",
    "model.add(GRU(nb_units))\n",
    "\n",
    "\n",
    "# To stack multiple RNN layers, all RNN layers except the last one need\n",
    "# to have \"return_sequences=True\".  An example of using two RNN layers:\n",
    "#model.add(SimpleRNN(16,\n",
    "#                    input_shape=(maxlen, len(chars)),\n",
    "#                    return_sequences=True))\n",
    "#model.add(SimpleRNN(32))\n",
    "\n",
    "model.add(Dense(units=len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_file = open(\"generated.txt\",\"w\")\n",
    "class SampleResult(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            \n",
    "            \n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print()\n",
    "            print('----- Generating with diversity',\n",
    "                  diversity, 'seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "            \n",
    "         \n",
    "\n",
    "            for i in range(100):\n",
    "                x = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = self.model.predict(x, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                generated_file.write(generated)\n",
    "                sys.stdout.flush()\n",
    "        print('\\n\\n')\n",
    "sample_callback = SampleResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 60s - loss: 3.2589\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"tana\n",
      "Zetes\"\n",
      "tana\n",
      "Zetesniaaa\n",
      "\n",
      "\n",
      "aaaaaailielaaae\n",
      "aalaaaaaaaaanaaaaaaaMaaaaaaaaiaa\n",
      "anleaaaaaaaaaiaaaaaaaaaeaaaaalaoaalalaaaa\n",
      "a\n",
      "----- Generating with diversity 0.5 seed: \"tana\n",
      "Zetes\"\n",
      "tana\n",
      "ZetesiaMe\n",
      "\n",
      "iaollndlnmaiiaeaaeiIaaopsoaal\n",
      "iPaalMPlmiaeelalpillnalliMealanirirmIaahi\n",
      "imaeaNamIla\n",
      "eaaieaaMMa\n",
      "----- Generating with diversity 1.0 seed: \"tana\n",
      "Zetes\"\n",
      "tana\n",
      "Zetes\n",
      "nTeratdctTdrrS\n",
      "nWtBStLukl\n",
      "nBrt\n",
      "BnifrBa\n",
      "\n",
      "igSBrhrdGpoMhBhmG\n",
      "gBageue\n",
      "tSabSeE\n",
      "uG\n",
      "fGra\n",
      "fa\n",
      "elntlnLimmMrmE\n",
      "----- Generating with diversity 1.2 seed: \"tana\n",
      "Zetes\"\n",
      "tana\n",
      "ZeteshehreelcrGftrtSa\n",
      "G\n",
      "ni\n",
      "eem\n",
      "nwhnnAanKpJlllmWMvPaClL\n",
      "vhiloleiJerrSaJeaiLBundrheear\n",
      "rhCKCy\n",
      "\n",
      "enailcoZah\n",
      "a\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      " - 57s - loss: 2.8977\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"orico\n",
      "Teod\"\n",
      "orico\n",
      "Teoda\n",
      "Mlaean\n",
      "Maaaaa\n",
      "Maiaaa\n",
      "Cenaaa\n",
      "Maeea\n",
      "Creaaa\n",
      "Mrarae\n",
      "Miarae\n",
      "Meoiee\n",
      "Caoaar\n",
      "Meaeaa\n",
      "Coaiaa\n",
      "Caraoa\n",
      "Maaaan\n",
      "M\n",
      "----- Generating with diversity 0.5 seed: \"orico\n",
      "Teod\"\n",
      "orico\n",
      "Teode\n",
      "Melesr\n",
      "Maani\n",
      "Caonal\n",
      "Mnorie\n",
      "Ceeare\n",
      "Riteer\n",
      "Cilann\n",
      "Tolean\n",
      "Rreoe\n",
      "Manrie\n",
      "Dalir\n",
      "Moleani\n",
      "Cnoruh\n",
      "Mooaao\n",
      "Mo\n",
      "----- Generating with diversity 1.0 seed: \"orico\n",
      "Teod\"\n",
      "orico\n",
      "Teodwd\n",
      "R\n",
      "irno\n",
      "Niaii\n",
      "Entreaan\n",
      "Meituizn\n",
      "Oon\n",
      "Mbeuotada\n",
      "Csnozxhps\n",
      "Mkeilt\n",
      "Miersuneyznns\n",
      "paaoaenhscf\n",
      "Tsx\n",
      "Liroc\n",
      "----- Generating with diversity 1.2 seed: \"orico\n",
      "Teod\"\n",
      "orico\n",
      "Teodo\n",
      "CarMin\n",
      "Raauh\n",
      "Rhhy\n",
      "Crneuair\n",
      "LgnNvclstaaness\n",
      "Msang\n",
      "Dnaooec\n",
      "Trnrtdi\n",
      "MnCuzgve\n",
      "Cum\n",
      "Dves\n",
      "aloeCayet\n",
      "Rauhu\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      " - 55s - loss: 2.5821\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"le\n",
      "Basilio\"\n",
      "le\n",
      "Basilio\n",
      "Baeiee\n",
      "Baoaee\n",
      "Baoaneene\n",
      "Ballea\n",
      "Boaaeae\n",
      "Boaleee\n",
      "Baanae\n",
      "Baaaeien\n",
      "Barree\n",
      "Baoaeee\n",
      "Baanaa\n",
      "Beaoiee\n",
      "Boarna\n",
      "----- Generating with diversity 0.5 seed: \"le\n",
      "Basilio\"\n",
      "le\n",
      "Basilion\n",
      "Bielyn\n",
      "Boeot\n",
      "Baioila\n",
      "Baieieiy\n",
      "Borriela\n",
      "Barltha\n",
      "Boenliete\n",
      "Soebete\n",
      "Siiloea\n",
      "Sounan\n",
      "Giaaoe\n",
      "Baoalies\n",
      "Ba\n",
      "----- Generating with diversity 1.0 seed: \"le\n",
      "Basilio\"\n",
      "le\n",
      "Basilior\n",
      "SagrCgmestmaegndtne\n",
      "Wotltoteo\n",
      "Sabolon\n",
      "Srelys\n",
      "Saioado\n",
      "Soatalerdtit\n",
      "Geoehoiksnloanehinnidre\n",
      "Oyllekto\n",
      "----- Generating with diversity 1.2 seed: \"le\n",
      "Basilio\"\n",
      "le\n",
      "Basilioadatecy\n",
      "Phausgn\n",
      "HhiltnoeE\n",
      "Uzia\n",
      "Namilern\n",
      "Ceomrziendnlhscclhamskathu\n",
      "Oanleompi\n",
      "Pouhsaeanndimaunstnsrtn\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      " - 61s - loss: 2.3681\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"aella\n",
      "Mich\"\n",
      "aella\n",
      "Michile\n",
      "Cilllin\n",
      "Ciilli\n",
      "Cillile\n",
      "Ciille\n",
      "Cililin\n",
      "Callile\n",
      "Ciilili\n",
      "Cililin\n",
      "Milile\n",
      "Cillin\n",
      "Cillile\n",
      "Ciile\n",
      "Rilina\n",
      "----- Generating with diversity 0.5 seed: \"aella\n",
      "Mich\"\n",
      "aella\n",
      "Michilin\n",
      "Cillis\n",
      "Nellime\n",
      "Milie\n",
      "Cileica\n",
      "Ralenin\n",
      "Rilline\n",
      "Rielo\n",
      "Rilile\n",
      "Miiile\n",
      "Riellon\n",
      "Niine\n",
      "Maneban\n",
      "Miila\n",
      "Ri\n",
      "----- Generating with diversity 1.0 seed: \"aella\n",
      "Mich\"\n",
      "aella\n",
      "Michasenhes\n",
      "Wonla\n",
      "Wobiirloy\n",
      "Hinnhir\n",
      "Hucgo\n",
      "Purrlhi\n",
      "Wareal\n",
      "Carlbda\n",
      "Clicceira\n",
      "Nililsaer\n",
      "Riidime\n",
      "Celktirs\n",
      "Ce\n",
      "----- Generating with diversity 1.2 seed: \"aella\n",
      "Mich\"\n",
      "aella\n",
      "Michinsfyri\n",
      "Wagiln\n",
      "ynote\n",
      "Pionae\n",
      "Filie\n",
      "Hesfilut\n",
      "Hisbe\n",
      "FuOrkone\n",
      "Pir\n",
      "Pitta\n",
      "Pelosye\n",
      "Higigig\n",
      "Wlirio\n",
      "Xlole\n",
      "Pid\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      " - 85s - loss: 2.2221\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"\n",
      "Ibbison\n",
      "I\"\n",
      "\n",
      "Ibbison\n",
      "Illle\n",
      "Lillan\n",
      "Lelllin\n",
      "Lillle\n",
      "Lilden\n",
      "Lillin\n",
      "Lilden\n",
      "Lilnen\n",
      "Lillin\n",
      "Lellle\n",
      "Lillla\n",
      "Lallli\n",
      "Lilllin\n",
      "Lillle\n",
      "Li\n",
      "----- Generating with diversity 0.5 seed: \"\n",
      "Ibbison\n",
      "I\"\n",
      "\n",
      "Ibbison\n",
      "Ildhar\n",
      "Ledns\n",
      "Lule\n",
      "Yann\n",
      "Lindte\n",
      "Linnsen\n",
      "Liinnt\n",
      "Lalne\n",
      "Linny\n",
      "Lilnd\n",
      "Odinn\n",
      "Hetne\n",
      "Ata\n",
      "Kesnn\n",
      "Shatn\n",
      "Sanlen\n",
      "San\n",
      "----- Generating with diversity 1.0 seed: \"\n",
      "Ibbison\n",
      "I\"\n",
      "\n",
      "Ibbison\n",
      "Inicudg\n",
      "Likanne\n",
      "Osdus\n",
      "Huxlreld\n",
      "Arran\n",
      "Keuu\n",
      "Sttp\n",
      "Syny\n",
      "Sthukdn\n",
      "Shaser\n",
      "Sthruhd\n",
      "Shetns\n",
      "KciryQekn\n",
      "Kemnn\n",
      "Spi\n",
      "----- Generating with diversity 1.2 seed: \"\n",
      "Ibbison\n",
      "I\"\n",
      "\n",
      "Ibbison\n",
      "Isnunbwarg\n",
      "Awyne\n",
      "Anhurta\n",
      "Kaf\n",
      "Sctesf\n",
      "Suary\n",
      "Sthrarme\n",
      "Swiura\n",
      "Sasn\n",
      "Starnt\n",
      "Ssoterdno\n",
      "Giinspknh\n",
      "Gahdnerscd\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      " - 85s - loss: 2.1059\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"Riha\n",
      "Rihan\"\n",
      "Riha\n",
      "Rihan\n",
      "Mice\n",
      "Milen\n",
      "Mille\n",
      "Milla\n",
      "Mille\n",
      "Mille\n",
      "Mille\n",
      "Mille\n",
      "Milla\n",
      "Mille\n",
      "Mille\n",
      "Mille\n",
      "Milla\n",
      "Mille\n",
      "Mille\n",
      "Mille\n",
      "Mill\n",
      "----- Generating with diversity 0.5 seed: \"Riha\n",
      "Rihan\"\n",
      "Riha\n",
      "Rihan\n",
      "Midliit\n",
      "Milnit\n",
      "Cilli\n",
      "Celle\n",
      "Cillel\n",
      "Milla\n",
      "Rille\n",
      "Mlile\n",
      "Mlile\n",
      "Milli\n",
      "Mille\n",
      "Milly\n",
      "Mille\n",
      "Mile\n",
      "Mila\n",
      "Milen\n",
      "M\n",
      "----- Generating with diversity 1.0 seed: \"Riha\n",
      "Rihan\"\n",
      "Riha\n",
      "Rihann\n",
      "Noih\n",
      "Caoe\n",
      "Coyte\n",
      "Maarke\n",
      "Coy\n",
      "Ramssa\n",
      "Momk\n",
      "Ralla\n",
      "Colli\n",
      "Mampa\n",
      "Meler\n",
      "Reibe\n",
      "Mocip\n",
      "Mola\n",
      "Rilma\n",
      "Cilerny\n",
      "Mila\n",
      "----- Generating with diversity 1.2 seed: \"Riha\n",
      "Rihan\"\n",
      "Riha\n",
      "Rihanka\n",
      "TegRha\n",
      "Mendih\n",
      "Mappia\n",
      "Codhico\n",
      "Momheld\n",
      "Mac\n",
      "Cememes\n",
      "Cabda\n",
      "Mlelry\n",
      "Tamg\n",
      "Rathika\n",
      "Rurida\n",
      "Mrerrifllon\n",
      "Hri\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      " - 59s - loss: 1.9771\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"ephira\n",
      "Sep\"\n",
      "ephira\n",
      "Sept\n",
      "Sente\n",
      "Sherte\n",
      "Ster\n",
      "Stert\n",
      "Surti\n",
      "Surti\n",
      "Surti\n",
      "Surter\n",
      "Surri\n",
      "Surti\n",
      "Surta\n",
      "Surti\n",
      "Surti\n",
      "Surti\n",
      "Surti\n",
      "Surst\n",
      "S\n",
      "----- Generating with diversity 0.5 seed: \"ephira\n",
      "Sep\"\n",
      "ephira\n",
      "Seper\n",
      "Shear\n",
      "Shenc\n",
      "Shera\n",
      "Sperte\n",
      "Stift\n",
      "Surta\n",
      "Surtul\n",
      "Surri\n",
      "Surtin\n",
      "Sural\n",
      "Srulr\n",
      "Surrisa\n",
      "Surre\n",
      "Surri\n",
      "Surta\n",
      "Su\n",
      "----- Generating with diversity 1.0 seed: \"ephira\n",
      "Sep\"\n",
      "ephira\n",
      "Sepreos\n",
      "Scensd\n",
      "Sihp\n",
      "Sentia\n",
      "Setpa\n",
      "Sepmiew\n",
      "Sherh\n",
      "Stercol\n",
      "Strrano\n",
      "Suusse\n",
      "Su\n",
      "Gurt\n",
      "Gruts\n",
      "Guurit\n",
      "Gutalte\n",
      "Gugt\n",
      "----- Generating with diversity 1.2 seed: \"ephira\n",
      "Sep\"\n",
      "ephira\n",
      "Sepss\n",
      "Siuhargepo\n",
      "Wuce\n",
      "Wusphice\n",
      "Wauza\n",
      "Wiwyia\n",
      "Whecid\n",
      "Fistesor\n",
      "Feticon\n",
      "Futlbilse\n",
      "Fuildta\n",
      "Funthdalta\n",
      "Pjui\n",
      "W\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      " - 53s - loss: 1.8377\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"alla\n",
      "Palla\"\n",
      "alla\n",
      "Pallan\n",
      "Wallelle\n",
      "Walliel\n",
      "Wallan\n",
      "Wallande\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wallelle\n",
      "Wa\n",
      "----- Generating with diversity 0.5 seed: \"alla\n",
      "Palla\"\n",
      "alla\n",
      "Pallante\n",
      "Palden\n",
      "Palean\n",
      "Paleon\n",
      "Paladal\n",
      "Paladorce\n",
      "Palitel\n",
      "Walperter\n",
      "Waduner\n",
      "Wodelan\n",
      "Wadestorie\n",
      "Zestice\n",
      "Zeso\n",
      "----- Generating with diversity 1.0 seed: \"alla\n",
      "Palla\"\n",
      "alla\n",
      "Pallaan\n",
      "Palbeled\n",
      "Poarbeot\n",
      "Pabola\n",
      "Wabbriesno\n",
      "Teorera\n",
      "Teorica\n",
      "Tecem\n",
      "Reildie\n",
      "Nebylra\n",
      "Rec\n",
      "Meilonco\n",
      "MMeo\n",
      "Reilb\n",
      "----- Generating with diversity 1.2 seed: \"alla\n",
      "Palla\"\n",
      "alla\n",
      "Pallansdidlar\n",
      "Yopewe\n",
      "Jahcy\n",
      "Yagmel\n",
      "Foadis\n",
      "Faena\n",
      "Folbie\n",
      "Faldallih\n",
      "Fameldia\n",
      "Faeemokel\n",
      "Faleluon\n",
      "Folconiamt\n",
      "Ge\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      " - 49s - loss: 1.6995\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"arolet\n",
      "Bar\"\n",
      "arolet\n",
      "Barler\n",
      "Barril\n",
      "Barrill\n",
      "Barrill\n",
      "Barrill\n",
      "Barrile\n",
      "Barrill\n",
      "Barriln\n",
      "Barrill\n",
      "Barriln\n",
      "Barrill\n",
      "Barriln\n",
      "Barriln\n",
      "B\n",
      "----- Generating with diversity 0.5 seed: \"arolet\n",
      "Bar\"\n",
      "arolet\n",
      "Barrin\n",
      "Barrallis\n",
      "Barrell\n",
      "Barrlid\n",
      "Barra\n",
      "Barre\n",
      "Barra\n",
      "Barrell\n",
      "Barlul\n",
      "Barrin\n",
      "Barrile\n",
      "Barrera\n",
      "Barrin\n",
      "Barril\n",
      "\n",
      "----- Generating with diversity 1.0 seed: \"arolet\n",
      "Bar\"\n",
      "arolet\n",
      "Barllen\n",
      "Barren\n",
      "Barttlin\n",
      "Brort\n",
      "Brromel\n",
      "Brarrla\n",
      "Brrlolttd\n",
      "Barlen\n",
      "Barrry\n",
      "Bralst\n",
      "Borren\n",
      "Borrera\n",
      "Barrine\n",
      "Bar\n",
      "----- Generating with diversity 1.2 seed: \"arolet\n",
      "Bar\"\n",
      "arolet\n",
      "Baros\n",
      "Bar\n",
      "Barvyn\n",
      "Baralend\n",
      "Borilleh\n",
      "Brorlell\n",
      "Brartp\n",
      "Brangtre\n",
      "Branczthe\n",
      "Brandicis\n",
      "Ba\n",
      "tryll\n",
      "Grricitttttiln\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      " - 55s - loss: 1.5841\n",
      "\n",
      "----- Generating with diversity 0.2 seed: \"erg\n",
      "Slotni\"\n",
      "erg\n",
      "Slotnie\n",
      "Solkel\n",
      "Solele\n",
      "Solele\n",
      "Solie\n",
      "Sole\n",
      "Sobelle\n",
      "Saolelle\n",
      "Saolelle\n",
      "Saolelle\n",
      "Saolella\n",
      "Saolelle\n",
      "Saonee\n",
      "Saone\n",
      "\n",
      "----- Generating with diversity 0.5 seed: \"erg\n",
      "Slotni\"\n",
      "erg\n",
      "Slotnie\n",
      "Slone\n",
      "Slanan\n",
      "Slanes\n",
      "Solie\n",
      "Salie\n",
      "Salerett\n",
      "Sabera\n",
      "Saboles\n",
      "Sabron\n",
      "Sabelin\n",
      "Sabelle\n",
      "Sabriee\n",
      "Sabron\n",
      "Sabe\n",
      "----- Generating with diversity 1.0 seed: \"erg\n",
      "Slotni\"\n",
      "erg\n",
      "Slotnis\n",
      "Slane\n",
      "Solpot\n",
      "Solster\n",
      "Samons\n",
      "Sami\n",
      "Sadiesteie\n",
      "Igetle\n",
      "Inneniy\n",
      "ihme\n",
      "Ziane\n",
      "Zeansed\n",
      "Fankbooj\n",
      "Fandees\n",
      "Fan\n",
      "----- Generating with diversity 1.2 seed: \"erg\n",
      "Slotni\"\n",
      "erg\n",
      "Slotninchd\n",
      "PawHahafse\n",
      "zamu\n",
      "Ioleka\n",
      "Iobys\n",
      "Iaxeyt\n",
      "Ianandamitzs\n",
      "Ynnovton\n",
      "Jonelna\n",
      "Jonasholi\n",
      "JormWFass\n",
      "Fawbhan\n",
      "F\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, \n",
    "                        epochs=10, \n",
    "                        batch_size=512,\n",
    "                        verbose=2,\n",
    "                       callbacks=[sample_callback])\n",
    "\n",
    "generated_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am\\nCramer\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
